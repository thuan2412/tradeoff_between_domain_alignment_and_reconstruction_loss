Args.output_dir is:  train_output
Args: Namespace(data_dir='~/data/', dataset='FullColoredMNIST', algorithm='IBIRM', hparams=None, hparams_seed=0, trial_seed=0, seed=0, steps=2001, checkpoint_freq=2000, test_envs=[0], output_dir='train_output', holdout_fraction=0.05, skip_model_save=False, lr=0.1, irm_lambda=None, irm_step=0, vrex_lambda=None, vrex_step=0, ib_lambda=None, ib_step=0, mmd_lambda=None, mmd_step=0, class_condition=False, normalize=False, dro_eta=None, bs=128, wd=0, sch_size=-1, d=0, debug=False, type=0, ratio=0.9, env_seed=1, no_aug=False, xyl=1, n_restarts=5, test_val=0)
Hparams: {'data_augmentation': True, 'resnet18': False, 'lr': 0.1, 'batch_size': 128, 'weight_decay': 0, 'class_balanced': False, 'resnet_dropout': 0.0, 'irm_lambda': 100.0, 'irm_penalty_anneal_iters': 0, 'vrex_lambda': 10.0, 'vrex_penalty_anneal_iters': 500, 'ib_lambda': 1, 'ib_penalty_anneal_iters': 0, 'mmd_lambda': 10.0, 'mmd_penalty_anneal_iters': 0, 'mixup_alpha': 0.2, 'groupdro_eta': 0.01, 'mmd_gamma': 1.0, 'mlp_width': 256, 'mlp_depth': 3, 'mlp_dropout': 0.0, 'mldg_beta': 1.0, 'mtl_ema': 0.99, 'sag_w_adv': 0.1, 'class_condition': False, 'normalize': False, 'xylopt': 1, 'xylnn': 1, 'dataset': 'FullColoredMNIST', 'sch_size': 600}
RRRRRRR Hyper sweep for IB-IRM
RRRRRRR args.seed:  0     args.trial_seed:  0
LRRRRRRR: 0.1
DECAY STEP: 600
env0_in  env0_ou  env1_in  env1_ou  env2_in  env2_ou  epoch    loss     nll      penalty  step     step_ti  train_a  val_acc  var     
0.11412  0.10034  0.11160  0.10891  0.11323  0.10205  0.00000  5.63511  2.30551  0.00005  0        1.00252  0.11241  0.10548  3.32213 
0.87292  0.84476  0.99824  0.99914  0.99228  0.98713  11.5486  0.27331  0.08401  0.02347  2000     0.11629  0.99526  0.99313  0.17737 
RRRRRRR args.seed:  1     args.trial_seed:  1
LRRRRRRR: 0.1
DECAY STEP: 600
env0_in  env0_ou  env1_in  env1_ou  env2_in  env2_ou  epoch    loss     nll      penalty  step     step_ti  train_a  val_acc  var     
0.09784  0.09777  0.09766  0.09176  0.09748  0.09262  0.00000  5.63751  2.29966  -0.0000  0        0.14495  0.09757  0.09219  3.32854 
0.85501  0.85934  0.99824  0.99656  0.99012  0.98199  11.5486  0.28126  0.08633  0.02386  2000     0.11670  0.99418  0.98928  0.18290 
RRRRRRR args.seed:  2     args.trial_seed:  2
LRRRRRRR: 0.1
DECAY STEP: 600
env0_in  env0_ou  env1_in  env1_ou  env2_in  env2_ou  epoch    loss     nll      penalty  step     step_ti  train_a  val_acc  var     
0.10280  0.09948  0.10019  0.10120  0.09992  0.09090  0.00000  5.60484  2.29149  0.00005  0        0.12893  0.10005  0.09605  3.30194 
0.86805  0.85420  0.99869  0.99571  0.99174  0.98370  11.5486  0.27933  0.08597  0.02382  2000     0.11885  0.99521  0.98970  0.18132 
RRRRRRR args.seed:  3     args.trial_seed:  3
LRRRRRRR: 0.1
DECAY STEP: 600
env0_in  env0_ou  env1_in  env1_ou  env2_in  env2_ou  epoch    loss     nll      penalty  step     step_ti  train_a  val_acc  var     
0.10506  0.10463  0.09825  0.10377  0.10272  0.11406  0.00000  5.73918  2.34405  0.00191  0        0.14395  0.10048  0.10891  3.38638 
0.86033  0.85506  0.99837  0.99914  0.99151  0.98456  11.5486  0.26780  0.08204  0.02310  2000     0.11912  0.99494  0.99185  0.17386 
RRRRRRR args.seed:  4     args.trial_seed:  4
LRRRRRRR: 0.1
DECAY STEP: 600
env0_in  env0_ou  env1_in  env1_ou  env2_in  env2_ou  epoch    loss     nll      penalty  step     step_ti  train_a  val_acc  var     
0.10470  0.10720  0.19917  0.18181  0.19429  0.17409  0.00000  5.63977  2.30865  0.00018  0        0.14489  0.19673  0.17795  3.32213 
