Environment:
	Python: 3.6.13
	PyTorch: 1.10.1
	Torchvision: 0.11.2
	CUDA: None
	CUDNN: None
	NumPy: 1.19.2
	PIL: 8.3.1
Args:
	algorithm: IB_IRM
	checkpoint_freq: None
	data_dir: None
	dataset: RotatedMNIST
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 64
	class_balanced: False
	data_augmentation: True
	ib_lambda: 100.0
	ib_penalty_anneal_iters: 500
	irm_lambda: 100.0
	irm_penalty_anneal_iters: 500
	lr: 0.001
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0
Environment:
	Python: 3.6.13
	PyTorch: 1.10.1
	Torchvision: 0.11.2
	CUDA: None
	CUDNN: None
	NumPy: 1.19.2
	PIL: 8.3.1
Args:
	algorithm: IB_IRM
	checkpoint_freq: None
	data_dir: None
	dataset: RotatedMNIST
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 64
	class_balanced: False
	data_augmentation: True
	ib_lambda: 100.0
	ib_penalty_anneal_iters: 500
	irm_lambda: 100.0
	irm_penalty_anneal_iters: 500
	lr: 0.001
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0
Environment:
	Python: 3.6.13
	PyTorch: 1.10.1
	Torchvision: 0.11.2
	CUDA: None
	CUDNN: None
	NumPy: 1.19.2
	PIL: 8.3.1
Args:
	algorithm: IB_IRM
	checkpoint_freq: None
	data_dir=: ./domainbed/data/
	dataset: RotatedMNIST
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 64
	class_balanced: False
	data_augmentation: True
	ib_lambda: 100.0
	ib_penalty_anneal_iters: 500
	irm_lambda: 100.0
	irm_penalty_anneal_iters: 500
	lr: 0.001
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0
Environment:
	Python: 3.6.13
	PyTorch: 1.10.1
	Torchvision: 0.11.2
	CUDA: None
	CUDNN: None
	NumPy: 1.19.2
	PIL: 8.3.1
Args:
	algorithm: IB_IRM
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: RotatedMNIST
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 64
	class_balanced: False
	data_augmentation: True
	ib_lambda: 100.0
	ib_penalty_anneal_iters: 500
	irm_lambda: 100.0
	irm_penalty_anneal_iters: 500
	lr: 0.001
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0
Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./domainbed/data/MNIST/raw/train-images-idx3-ubyte.gz
  0%|          | 0/9912422 [00:00<?, ?it/s]  6%|5         | 549888/9912422 [00:00<00:01, 5498872.13it/s] 14%|#3        | 1387520/9912422 [00:00<00:01, 7191158.75it/s] 28%|##7       | 2728960/9912422 [00:00<00:00, 9914502.37it/s] 37%|###7      | 3717120/9912422 [00:00<00:00, 9473442.21it/s] 53%|#####2    | 5217280/9912422 [00:00<00:00, 11293505.44it/s] 68%|######8   | 6751232/9912422 [00:00<00:00, 12546172.70it/s] 84%|########3 | 8285184/9912422 [00:00<00:00, 13318872.95it/s] 97%|#########7| 9620480/9912422 [00:00<00:00, 12243940.01it/s]9913344it [00:00, 11596410.26it/s]                             
Extracting ./domainbed/data/MNIST/raw/train-images-idx3-ubyte.gz to ./domainbed/data/MNIST/raw

Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./domainbed/data/MNIST/raw/train-labels-idx1-ubyte.gz
  0%|          | 0/28881 [00:00<?, ?it/s]29696it [00:00, 5801306.55it/s]          
Extracting ./domainbed/data/MNIST/raw/train-labels-idx1-ubyte.gz to ./domainbed/data/MNIST/raw

Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./domainbed/data/MNIST/raw/t10k-images-idx3-ubyte.gz
  0%|          | 0/1648877 [00:00<?, ?it/s] 70%|######9   | 1147904/1648877 [00:00<00:00, 11288606.35it/s]1649664it [00:00, 8724919.00it/s]                              
Extracting ./domainbed/data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./domainbed/data/MNIST/raw

Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz
Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./domainbed/data/MNIST/raw/t10k-labels-idx1-ubyte.gz
  0%|          | 0/4542 [00:00<?, ?it/s]5120it [00:00, 2841338.51it/s]          
Extracting ./domainbed/data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./domainbed/data/MNIST/raw

IB_penalty    IRM_penalty   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  env4_in_acc   env4_out_acc  env5_in_acc   env5_out_acc  epoch         loss          mem_gb        nll           step          step_time    
3.3544380665  0.0024100975  0.0966359546  0.0985855122  0.0973859010  0.1084440634  0.0963145490  0.0938705529  0.1015641740  0.0951564509  0.0998607093  0.0981568796  0.0972891889  0.0964423489  0.0000000000  2.3197731972  0.0000000000  2.3197731972  0             5.0824699402 
Environment:
	Python: 3.6.13
	PyTorch: 1.10.1
	Torchvision: 0.11.2
	CUDA: None
	CUDNN: None
	NumPy: 1.19.2
	PIL: 8.3.1
Args:
	algorithm: IB_IRM
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: PACS
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 32
	class_balanced: False
	data_augmentation: True
	ib_lambda: 100.0
	ib_penalty_anneal_iters: 500
	irm_lambda: 100.0
	irm_penalty_anneal_iters: 500
	lr: 5e-05
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0
Downloading: "https://download.pytorch.org/models/resnet50-0676ba61.pth" to /Users/thuannguyen/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth
  0%|          | 0.00/97.8M [00:00<?, ?B/s]  1%|          | 640k/97.8M [00:00<00:15, 6.55MB/s]  2%|1         | 1.80M/97.8M [00:00<00:11, 8.76MB/s]  3%|3         | 3.17M/97.8M [00:00<00:10, 9.12MB/s]  5%|4         | 4.51M/97.8M [00:00<00:09, 10.7MB/s]  6%|5         | 5.55M/97.8M [00:00<00:10, 9.28MB/s]  7%|6         | 6.66M/97.8M [00:00<00:09, 9.91MB/s]  8%|7         | 7.82M/97.8M [00:00<00:08, 10.5MB/s]  9%|9         | 8.86M/97.8M [00:00<00:09, 9.42MB/s] 10%|#         | 10.0M/97.8M [00:01<00:09, 10.1MB/s] 11%|#1        | 11.0M/97.8M [00:01<00:10, 9.04MB/s] 12%|#2        | 12.0M/97.8M [00:01<00:10, 8.52MB/s] 14%|#3        | 13.4M/97.8M [00:01<00:08, 10.1MB/s] 15%|#4        | 14.4M/97.8M [00:01<00:09, 8.78MB/s] 16%|#6        | 15.8M/97.8M [00:01<00:08, 10.3MB/s] 17%|#7        | 16.9M/97.8M [00:01<00:09, 9.24MB/s] 18%|#8        | 17.9M/97.8M [00:02<00:10, 8.04MB/s] 20%|#9        | 19.2M/97.8M [00:02<00:09, 8.41MB/s] 21%|##        | 20.2M/97.8M [00:02<00:09, 8.92MB/s] 22%|##1       | 21.1M/97.8M [00:02<00:10, 7.90MB/s] 23%|##2       | 22.4M/97.8M [00:02<00:08, 9.33MB/s] 24%|##3       | 23.4M/97.8M [00:02<00:08, 9.39MB/s] 25%|##4       | 24.4M/97.8M [00:02<00:10, 7.59MB/s] 26%|##6       | 25.5M/97.8M [00:03<00:10, 7.48MB/s] 28%|##7       | 27.0M/97.8M [00:03<00:08, 9.26MB/s] 29%|##8       | 28.0M/97.8M [00:03<00:10, 7.13MB/s] 30%|###       | 29.4M/97.8M [00:03<00:08, 8.69MB/s] 31%|###1      | 30.4M/97.8M [00:03<00:08, 7.94MB/s] 32%|###2      | 31.6M/97.8M [00:03<00:08, 8.13MB/s] 33%|###3      | 32.5M/97.8M [00:03<00:09, 6.97MB/s] 34%|###4      | 33.7M/97.8M [00:04<00:09, 7.07MB/s] 36%|###5      | 34.8M/97.8M [00:04<00:08, 7.92MB/s] 37%|###7      | 36.2M/97.8M [00:04<00:06, 9.33MB/s] 38%|###8      | 37.2M/97.8M [00:04<00:08, 7.15MB/s] 39%|###9      | 38.6M/97.8M [00:04<00:07, 8.67MB/s] 40%|####      | 39.6M/97.8M [00:04<00:09, 6.71MB/s] 42%|####1     | 40.7M/97.8M [00:05<00:11, 5.11MB/s] 43%|####3     | 42.5M/97.8M [00:05<00:08, 7.18MB/s] 44%|####4     | 43.5M/97.8M [00:05<00:08, 6.83MB/s] 46%|####6     | 45.0M/97.8M [00:05<00:06, 8.46MB/s] 47%|####7     | 46.0M/97.8M [00:06<00:08, 6.27MB/s] 48%|####7     | 46.9M/97.8M [00:06<00:08, 6.35MB/s] 49%|####8     | 47.6M/97.8M [00:06<00:08, 6.14MB/s] 50%|####9     | 48.6M/97.8M [00:06<00:07, 6.76MB/s] 51%|#####     | 49.8M/97.8M [00:06<00:06, 7.98MB/s] 52%|#####1    | 50.6M/97.8M [00:06<00:08, 5.53MB/s] 53%|#####3    | 52.0M/97.8M [00:06<00:06, 7.13MB/s] 55%|#####4    | 53.3M/97.8M [00:07<00:05, 8.54MB/s] 56%|#####6    | 54.8M/97.8M [00:07<00:04, 10.1MB/s] 57%|#####7    | 56.0M/97.8M [00:07<00:05, 8.23MB/s] 58%|#####8    | 56.9M/97.8M [00:07<00:04, 8.61MB/s] 59%|#####9    | 58.1M/97.8M [00:07<00:04, 9.15MB/s] 60%|######    | 59.1M/97.8M [00:07<00:04, 8.37MB/s] 62%|######1   | 60.4M/97.8M [00:07<00:05, 7.52MB/s] 63%|######2   | 61.4M/97.8M [00:08<00:04, 7.73MB/s] 64%|######3   | 62.2M/97.8M [00:08<00:06, 5.51MB/s] 65%|######4   | 63.5M/97.8M [00:08<00:05, 7.07MB/s] 66%|######5   | 64.4M/97.8M [00:08<00:05, 6.48MB/s] 67%|######6   | 65.5M/97.8M [00:08<00:05, 6.15MB/s] 68%|######8   | 66.6M/97.8M [00:08<00:04, 7.18MB/s] 69%|######9   | 67.8M/97.8M [00:09<00:04, 7.65MB/s] 70%|#######   | 68.7M/97.8M [00:09<00:03, 7.95MB/s] 71%|#######1  | 69.6M/97.8M [00:09<00:04, 6.22MB/s] 72%|#######2  | 70.5M/97.8M [00:09<00:04, 5.78MB/s] 74%|#######4  | 72.5M/97.8M [00:09<00:03, 8.75MB/s] 75%|#######5  | 73.5M/97.8M [00:09<00:03, 7.44MB/s] 76%|#######6  | 74.4M/97.8M [00:10<00:05, 4.55MB/s] 77%|#######7  | 75.7M/97.8M [00:10<00:03, 5.91MB/s] 78%|#######8  | 76.6M/97.8M [00:10<00:03, 6.07MB/s] 80%|########  | 78.4M/97.8M [00:10<00:02, 7.82MB/s] 81%|########1 | 79.3M/97.8M [00:10<00:02, 7.88MB/s] 82%|########2 | 80.3M/97.8M [00:10<00:02, 8.46MB/s] 83%|########3 | 81.2M/97.8M [00:11<00:01, 8.68MB/s] 84%|########4 | 82.2M/97.8M [00:11<00:01, 8.77MB/s] 85%|########5 | 83.1M/97.8M [00:11<00:01, 9.05MB/s] 87%|########6 | 84.9M/97.8M [00:11<00:01, 9.63MB/s] 88%|########7 | 85.8M/97.8M [00:11<00:01, 8.84MB/s] 89%|########8 | 87.0M/97.8M [00:11<00:01, 7.89MB/s] 90%|########9 | 88.0M/97.8M [00:11<00:01, 8.49MB/s] 92%|#########1| 89.6M/97.8M [00:11<00:00, 10.4MB/s] 93%|#########2| 90.7M/97.8M [00:12<00:00, 7.71MB/s] 94%|#########3| 91.6M/97.8M [00:12<00:01, 4.64MB/s] 96%|#########5| 93.6M/97.8M [00:12<00:00, 6.19MB/s] 97%|#########6| 94.5M/97.8M [00:13<00:00, 5.90MB/s] 98%|#########7| 95.5M/97.8M [00:13<00:00, 6.53MB/s] 99%|#########8| 96.8M/97.8M [00:13<00:00, 7.73MB/s]100%|#########9| 97.7M/97.8M [00:13<00:00, 7.10MB/s]100%|##########| 97.8M/97.8M [00:13<00:00, 7.61MB/s]
IB_penalty    IRM_penalty   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  env3_in_acc   env3_out_acc  epoch         loss          mem_gb        nll           step          step_time    
2.9813280106  0.0098829689  0.2080536913  0.2029339853  0.3017057569  0.3119658120  0.3121257485  0.2784431138  0.2442748092  0.2305732484  0.0000000000  2.0073432922  0.0000000000  2.0073432922  0             46.576906919 
Environment:
	Python: 3.6.13
	PyTorch: 1.10.1
	Torchvision: 0.11.2
	CUDA: None
	CUDNN: None
	NumPy: 1.19.2
	PIL: 8.3.1
Args:
	algorithm: IB_IRM
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: ColoredMNIST
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 64
	class_balanced: False
	data_augmentation: True
	ib_lambda: 100.0
	ib_penalty_anneal_iters: 500
	irm_lambda: 100.0
	irm_penalty_anneal_iters: 500
	lr: 0.001
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0
IB_penalty    IRM_penalty   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  epoch         loss          mem_gb        nll           step          step_time    
1.0733134747  0.0045223744  0.4972680523  0.4976425204  0.4996517919  0.4952850407  0.4963839931  0.4939991427  0.0000000000  0.7318947911  0.0000000000  0.7318947911  0             2.2813198566 
1.0551094925  0.0648330070  0.1033319049  0.1063009001  0.2050141962  0.1952421775  0.9008946269  0.9061294471  0.3428510205  0.7809252971  0.0000000000  0.7809252971  100           0.6392491174 
1.0125232083  0.0013152957  0.1498821513  0.1489498500  0.2431563722  0.2383197600  0.8831628007  0.8859837120  0.6857020410  0.6877849847  0.0000000000  0.6877849847  200           0.6461415529 
1.0094100124  0.0017736083  0.2879258624  0.2809687098  0.3506723094  0.3516930990  0.8066641667  0.8030432919  1.0285530616  0.6846862942  0.0000000000  0.6846862942  300           0.6404713392 
0.9994224346  0.0019769068  0.3112813370  0.3122588941  0.3696898270  0.3669095585  0.8292709059  0.8319759966  1.3714040821  0.6785876811  0.0000000000  0.6785876811  400           0.6076086903 
0.9803841496  0.0020424785  0.5027319477  0.5023574796  0.5003482081  0.5047149593  0.5036160069  0.5060008573  1.7142551026  1.6268212092  0.0000000000  1.6268212092  500           0.6165418482 
0.9800265414  0.0051596812  0.6100278552  0.6086583798  0.6187925216  0.6195885126  0.7248620560  0.7233176168  2.0571061231  99.193556289  0.0000000000  99.193556289  600           0.6012012935 
0.8940986568  0.0019581039  0.7032890508  0.7036005144  0.7063802432  0.7029575654  0.7167193443  0.7141020146  2.3999571436  90.217276001  0.0000000000  90.217276001  700           0.6008251047 
0.8695464242  0.0017075321  0.6872723377  0.6892413202  0.6948090213  0.6817402486  0.7437724326  0.7400342906  2.7428081641  87.720658493  0.0000000000  87.720658493  800           0.6029687572 
0.8595351827  0.0010506389  0.6930040711  0.6967423918  0.7007553437  0.6973853408  0.7489151979  0.7458208315  3.0856591847  86.647873382  0.0000000000  86.647873382  900           0.6042405725 
0.8511239773  0.0019109168  0.6995928862  0.7033861980  0.7066480956  0.6986712387  0.7523972786  0.7477496785  3.4285102052  85.887579879  0.0000000000  85.887579879  1000          0.6038776970 
0.8491902637  0.0016238041  0.6807370902  0.6793827690  0.6935233299  0.6877411059  0.7587721648  0.7516073725  3.7713612257  85.664679946  0.0000000000  85.664679946  1100          0.6041088128 
0.8284968668  0.0003597181  0.7152881937  0.7134590656  0.7211121230  0.7173167595  0.7486473456  0.7406772396  4.1142122462  83.454556884  0.0000000000  83.454556884  1200          0.6019786263 
0.8497807056  0.0002310774  0.7184486822  0.7216030862  0.7246477741  0.7153879126  0.7443081374  0.7359622803  4.4570632667  85.585686416  0.0000000000  85.585686416  1300          0.6016997600 
0.8362132770  0.0016397003  0.7080029998  0.7102443206  0.7165586329  0.7089584226  0.7533079766  0.7449635662  4.7999142872  84.360522155  0.0000000000  84.360522155  1400          0.5990585184 
0.8367953253  0.0003758608  0.7175916006  0.7183883412  0.7233620828  0.7143163309  0.7480044999  0.7370338620  5.1427653078  84.292957458  0.0000000000  84.292957458  1500          0.5988885546 
0.8170990604  -0.000004409  0.7242875509  0.7271753108  0.7310762308  0.7192456065  0.7478437885  0.7393913416  5.4856163283  82.271695175  0.0000000000  82.271695175  1600          0.5997973824 
0.8202097917  0.0006997720  0.7191450611  0.7162451779  0.7283441367  0.7138876982  0.7489687684  0.7366052293  5.8284673488  82.655493354  0.0000000000  82.655493354  1700          0.6053192163 
0.8177552032  0.0006318263  0.7241804157  0.7224603515  0.7331654792  0.7233176168  0.7476830771  0.7381054436  6.1713183693  82.401675872  0.0000000000  82.401675872  1800          0.6006390476 
0.8201202917  0.0009505253  0.7160381401  0.7177453922  0.7290941233  0.7119588513  0.7496651846  0.7396056580  6.5141693898  82.671909332  0.0000000000  82.671909332  1900          0.6015180230 
0.8190144247  0.0005075379  0.7163059781  0.7198885555  0.7288262710  0.7145306472  0.7560400707  0.7453921989  6.8570204103  82.516353530  0.0000000000  82.516353530  2000          0.6007160830 
0.8230286300  0.0018750917  0.7124491108  0.7143163309  0.7263084588  0.7130304329  0.7560936412  0.7423917703  7.1998714309  83.057503738  0.0000000000  83.057503738  2100          0.6024059296 
0.8091237909  0.0013101867  0.7135740304  0.7158165452  0.7281298548  0.7111015859  0.7604864199  0.7451778826  7.5427224514  81.600930023  0.0000000000  81.600930023  2200          0.6014483833 
0.8097166383  0.0016467585  0.7054853225  0.7083154736  0.7260406064  0.7053150450  0.7583436010  0.7393913416  7.8855734719  81.694385376  0.0000000000  81.694385376  2300          0.5985056162 
0.7990389967  0.0013020248  0.7081637026  0.7143163309  0.7275941501  0.7102443206  0.7636470777  0.7456065152  8.2284244924  80.584841232  0.0000000000  80.584841232  2400          0.6002913046 
0.8032224953  0.0013665678  0.7102528391  0.7098156880  0.7280762844  0.7023146164  0.7477366476  0.7273896271  8.5712755129  81.012499542  0.0000000000  81.012499542  2500          0.6004639363 
0.8034441251  0.0008197519  0.7111099207  0.7098156880  0.7332726201  0.7087441063  0.7629506616  0.7468924132  8.9141265335  80.980349960  0.0000000000  80.980349960  2600          0.6011078906 
0.8035417819  0.0022122986  0.7230554960  0.7207458208  0.7426474527  0.7196742392  0.7580757486  0.7396056580  9.2569775540  81.129494857  0.0000000000  81.129494857  2700          0.6006040049 
0.7968083858  0.0018139595  0.7091814870  0.7141020146  0.7324690631  0.7098156880  0.7570043392  0.7359622803  9.5998285745  80.411656189  0.0000000000  80.411656189  2800          0.6012123585 
0.7896784550  0.0013695276  0.7131454896  0.7136733819  0.7402903520  0.7153879126  0.7657363261  0.7434633519  9.9426795950  79.649320983  0.0000000000  79.649320983  2900          0.6011489272 
0.7863357019  0.0000846630  0.7089136490  0.7121731676  0.7390046606  0.7100300043  0.7619863931  0.7346763823  10.285530615  79.184173126  0.0000000000  79.184173126  3000          0.6009534645 
0.7905547005  0.0002498437  0.7057531605  0.7059579940  0.7391118016  0.7051007287  0.7697541115  0.7423917703  10.628381636  79.625701904  0.0000000000  79.625701904  3100          0.5995075130 
0.7766312420  0.0010927457  0.7160381401  0.7175310759  0.7477902180  0.7158165452  0.7664327423  0.7398199743  10.971232656  78.307997207  0.0000000000  78.307997207  3200          0.6005881023 
0.7817066163  0.0008332314  0.6963788301  0.7025289327  0.7302191032  0.7003857694  0.7621471045  0.7327475354  11.314083677  78.793212890  0.0000000000  78.793212890  3300          0.6000917387 
0.7688643068  0.0018840579  0.6991643454  0.7051007287  0.7445759897  0.7006000857  0.7777896823  0.7430347192  11.656934697  77.605108223  0.0000000000  77.605108223  3400          0.6016347051 
0.7624733347  0.0017515650  0.6754874652  0.6682383198  0.7339690363  0.6821688813  0.7841109980  0.7393913416  11.999785718  76.948285713  0.0000000000  76.948285713  3500          0.6009456658 
0.7645867091  0.0008922497  0.6978787229  0.6980282898  0.7478437885  0.6997428204  0.7734504741  0.7291041577  12.342636738  77.075021972  0.0000000000  77.075021972  3600          0.6005383873 
0.7568077976  0.0013197806  0.7232697664  0.7246035148  0.7623078159  0.7218174025  0.7537365404  0.7111015859  12.685487759  76.334471092  0.0000000000  76.334471092  3700          0.5981025887 
0.7484311181  0.0001263370  0.7004499679  0.6980282898  0.7549686613  0.7057436777  0.7722183532  0.7288898414  13.028338779  75.371631355  0.0000000000  75.371631355  3800          0.6004797983 
0.7574075752  0.0009877625  0.6790229269  0.6780968710  0.7393260835  0.6806686670  0.7759682863  0.7226746678  13.371189800  76.361607513  0.0000000000  76.361607513  3900          0.6012744284 
0.7323695356  0.0013585525  0.7067709449  0.7089584226  0.7663791718  0.7046720960  0.7835752933  0.7271753108  13.714040820  73.877529792  0.0000000000  73.877529792  4000          0.6015717340 
0.7287910390  0.0015066104  0.7075744590  0.7081011573  0.7793967965  0.7091727390  0.7760754272  0.7059579940  14.056891841  73.531891975  0.0000000000  73.531891975  4100          0.6016860676 
0.7151360303  0.0007285189  0.6799335762  0.6793827690  0.7606471313  0.6838834119  0.7953607971  0.7299614231  14.399742861  72.078908729  0.0000000000  72.078908729  4200          0.6083449411 
0.7068098420  0.0008364652  0.7002892651  0.7031718817  0.7778968233  0.6982426061  0.7788075213  0.7070295757  14.742593882  71.251406135  0.0000000000  71.251406135  4300          0.6014766240 
0.7010654545  -0.001124671  0.6875937433  0.6911701672  0.7775754004  0.6881697385  0.7785932394  0.6975996571  15.085444902  70.476837348  0.0000000000  70.476837348  4400          0.6004939675 
0.6873762339  0.0010816747  0.6735054639  0.6858122589  0.7858788236  0.6774539220  0.8002892805  0.7138876982  15.428295923  69.318959045  0.0000000000  69.318959045  4500          0.5989035964 
0.6830691040  0.0011653886  0.6727019499  0.6735962280  0.7804682059  0.6791684526  0.8123962072  0.7291041577  15.771146943  68.893599166  0.0000000000  68.893599166  4600          0.5989585328 
0.6591791195  0.0015787658  0.6705592458  0.6759537077  0.7887180586  0.6793827690  0.8164675631  0.7183883412  16.113997964  66.529244041  0.0000000000  66.529244041  4700          0.5988352871 
0.6462856081  0.0021736485  0.6695414613  0.6742391770  0.8006107034  0.6746678097  0.8221460331  0.7128161166  16.456848984  65.290268135  0.0000000000  65.290268135  4800          0.5986102867 
0.6265841198  0.0002918052  0.6599528605  0.6663094728  0.8015214014  0.6667381054  0.8312530133  0.7096013716  16.799700005  63.118144340  0.0000000000  63.118144340  4900          0.5988715053 
0.6229844236  0.0002251672  0.6618277266  0.6588084012  0.7896287566  0.6671667381  0.8378957519  0.7318902700  17.142551025  62.749000206  0.0000000000  62.749000206  5000          0.5981640792 
Environment:
	Python: 3.6.13
	PyTorch: 1.10.1
	Torchvision: 0.11.2
	CUDA: None
	CUDNN: None
	NumPy: 1.19.2
	PIL: 8.3.1
Args:
	algorithm: ERM
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: ColoredMNIST
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 64
	class_balanced: False
	data_augmentation: True
	lr: 0.001
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  epoch         loss          mem_gb        step          step_time    
0.4972680523  0.4976425204  0.4996517919  0.4952850407  0.4963839931  0.4939991427  0.0000000000  0.7273724079  0.0000000000  0             2.0951392651 
0.1017784444  0.1039434205  0.2032999411  0.1935276468  0.9013231907  0.9063437634  0.3428510205  0.7133719891  0.0000000000  100           0.6357582331 
0.3435290336  0.3441920274  0.3907430224  0.3964852122  0.7290941233  0.7385340763  0.6857020410  0.6842233276  0.0000000000  200           0.6428026175 
0.6415791729  0.6311615945  0.6386135962  0.6414487784  0.6597739326  0.6622374625  1.0285530616  0.6707057589  0.0000000000  300           0.6541778183 
0.5946539533  0.5932276039  0.6182032464  0.6178739820  0.7691112659  0.7653236177  1.3714040821  0.6196517771  0.0000000000  400           0.6423380327 
0.6746303835  0.6746678097  0.6826485241  0.6838834119  0.7476830771  0.7434633519  1.7142551026  0.5953988099  0.0000000000  500           0.6471685481 
0.7118598672  0.7123874839  0.7181121766  0.7108872696  0.7425938823  0.7378911273  2.0571061231  0.5821819031  0.0000000000  600           0.6547712517 
0.7104671095  0.7111015859  0.7142015321  0.7038148307  0.7405582043  0.7338191170  2.3999571436  0.5807272878  0.0000000000  700           0.6440250683 
0.6934326120  0.6913844835  0.7049874109  0.6896699529  0.7581828896  0.7518216888  2.7428081641  0.5804565847  0.0000000000  800           0.6154574656 
0.7015213199  0.7038148307  0.7109873038  0.7018859837  0.7547008089  0.7471067295  3.0856591847  0.5779774234  0.0000000000  900           0.6126938701 
0.7084315406  0.7093870553  0.7180586061  0.7078868410  0.7533079766  0.7458208315  3.4285102052  0.5759072322  0.0000000000  1000          0.6089582682 
0.6958967217  0.6954564938  0.7082552097  0.7027432490  0.7597364333  0.7473210459  3.7713612257  0.5777582091  0.0000000000  1100          0.6084557962 
0.7116991643  0.7113159023  0.7193978679  0.7063866267  0.7549150908  0.7486069438  4.1142122462  0.5635807544  0.0000000000  1200          0.6085017133 
0.7153417613  0.7151735962  0.7232013714  0.7126018003  0.7505223121  0.7423917703  4.4570632667  0.5783387691  0.0000000000  1300          0.6091517949 
0.7054853225  0.7081011573  0.7153265120  0.7051007287  0.7598971447  0.7503214745  4.7999142872  0.5697308961  0.0000000000  1400          0.6117185998 
0.7080565674  0.7096013716  0.7234156533  0.7081011573  0.7523437081  0.7430347192  5.1427653078  0.5709948319  0.0000000000  1500          0.6118635559 
0.7183415470  0.7205315045  0.7293084052  0.7151735962  0.7541651042  0.7436776682  5.4856163283  0.5564520493  0.0000000000  1600          0.6090595007 
0.7212341976  0.7211744535  0.7310226603  0.7177453922  0.7503616007  0.7366052293  5.8284673488  0.5578791240  0.0000000000  1700          0.6086390424 
0.7242339833  0.7276039434  0.7347190229  0.7203171882  0.7515401511  0.7398199743  6.1713183693  0.5602180618  0.0000000000  1800          0.6082990003 
0.7110027855  0.7089584226  0.7255584722  0.7087441063  0.7605935608  0.7486069438  6.5141693898  0.5604852763  0.0000000000  1900          0.6085057044 
0.7166273838  0.7209601372  0.7297905395  0.7132447492  0.7594685809  0.7477496785  6.8570204103  0.5603698078  0.0000000000  2000          0.6103254533 
0.7130919220  0.7138876982  0.7286655595  0.7132447492  0.7601649971  0.7453921989  7.1998714309  0.5622599745  0.0000000000  2100          0.6082891273 
0.7098778659  0.7136733819  0.7281834253  0.7087441063  0.7651470509  0.7496785255  7.5427224514  0.5539891362  0.0000000000  2200          0.6084081411 
0.7068780801  0.7102443206  0.7323083516  0.7055293613  0.7609685541  0.7400342906  7.8855734719  0.5529162520  0.0000000000  2300          0.6084405327 
0.7131990572  0.7126018003  0.7308619489  0.7168881269  0.7652006214  0.7511787398  8.2284244924  0.5481437895  0.0000000000  2400          0.6091817474 
0.6923612599  0.6967423918  0.7161300691  0.6898842692  0.7671291584  0.7436776682  8.5712755129  0.5506804848  0.0000000000  2500          0.6086049747 
0.7108420827  0.7115302186  0.7312369422  0.7051007287  0.7656291852  0.7498928418  8.9141265335  0.5514402968  0.0000000000  2600          0.6098508644 
0.7211270624  0.7233176168  0.7416831842  0.7143163309  0.7604864199  0.7404629233  9.2569775540  0.5521019629  0.0000000000  2700          0.6083981872 
0.6924148275  0.7023146164  0.7252370493  0.6993141877  0.7643434939  0.7456065152  9.5998285745  0.5437185606  0.0000000000  2800          0.6091517663 
0.7043068352  0.7042434634  0.7351475867  0.7057436777  0.7712540847  0.7451778826  9.9426795950  0.5368492788  0.0000000000  2900          0.6089278150 
0.6961645597  0.6963137591  0.7311298013  0.6967423918  0.7735576150  0.7466780969  10.285530615  0.5398091522  0.0000000000  3000          0.6091202688 
0.6892007714  0.6866695242  0.7269513044  0.6911701672  0.7770396957  0.7447492499  10.628381636  0.5416763231  0.0000000000  3100          0.6083914304 
0.7069852153  0.7117445349  0.7467188086  0.7053150450  0.7653613328  0.7323189027  10.971232656  0.5300174844  0.0000000000  3200          0.6089787507 
0.6996464538  0.6995285041  0.7368618418  0.6975996571  0.7690576954  0.7363909130  11.314083677  0.5316517419  0.0000000000  3300          0.6100715423 
0.6797728734  0.6864552079  0.7316119355  0.6813116159  0.7918787165  0.7550364338  11.656934697  0.5272810242  0.0000000000  3400          0.6128220940 
0.6585065352  0.6585940849  0.7201478545  0.6665237891  0.7946108105  0.7516073725  11.999785718  0.5203467047  0.0000000000  3500          0.6093918300 
0.6842725520  0.6870981569  0.7394867949  0.6883840549  0.7875395082  0.7417488213  12.342636738  0.5201815906  0.0000000000  3600          0.6097294354 
0.7239125777  0.7222460351  0.7676648631  0.7166738105  0.7642899234  0.7196742392  12.685487759  0.5115772524  0.0000000000  3700          0.6103812027 
0.6924148275  0.7012430347  0.7552900841  0.6920274325  0.7827181657  0.7239605658  13.028338779  0.5064999026  0.0000000000  3800          0.6099433279 
0.6725412471  0.6793827690  0.7503080302  0.6793827690  0.8009321262  0.7423917703  13.371189800  0.5126059002  0.0000000000  3900          0.6099059081 
0.6975037497  0.7046720960  0.7672898698  0.7010287184  0.7895216157  0.7346763823  13.714040820  0.4948518229  0.0000000000  4000          0.6107043695 
0.6900578530  0.6930990141  0.7705040981  0.6838834119  0.7963250656  0.7216030862  14.056891841  0.4944786909  0.0000000000  4100          0.6194944215 
0.6759160060  0.6753107587  0.7715755076  0.6768109730  0.8091284084  0.7402486069  14.399742861  0.4802031288  0.0000000000  4200          0.6094282293 
0.6853439040  0.6907415345  0.7769861252  0.6851693099  0.7995392939  0.7149592799  14.742593882  0.4711818150  0.0000000000  4300          0.6398416853 
0.6633276195  0.6673810544  0.7772004071  0.6714530647  0.8097176836  0.7162451779  15.085444902  0.4645409712  0.0000000000  4400          0.6370930982 
0.6673451896  0.6630947278  0.7814324744  0.6665237891  0.8221460331  0.7158165452  15.428295923  0.4639766303  0.0000000000  4500          0.6396959734 
0.6309727877  0.6378054008  0.7634327958  0.6423060437  0.8330208389  0.7301757394  15.771146943  0.4572695255  0.0000000000  4600          0.6411348915 
0.6357938719  0.6345906558  0.7797182193  0.6519502786  0.8470027321  0.7312473210  16.113997964  0.4370026141  0.0000000000  4700          0.6362122273 
0.6327940861  0.6337333905  0.7813789039  0.6504500643  0.8513955108  0.7378911273  16.456848984  0.4252427137  0.0000000000  4800          0.6634036946 
0.6467216627  0.6508786970  0.8043070659  0.6517359623  0.8516633632  0.7254607801  16.799700005  0.4155829176  0.0000000000  4900          0.6506262112 
0.6335976002  0.6427346764  0.8040927841  0.6412344621  0.8622703166  0.7297471067  17.142551025  0.4131208345  0.0000000000  5000          0.6478152895 
Environment:
	Python: 3.6.13
	PyTorch: 1.10.1
	Torchvision: 0.11.2
	CUDA: None
	CUDNN: None
	NumPy: 1.19.2
	PIL: 8.3.1
Args:
	algorithm: ERM
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: ColoredMNIST
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 64
	class_balanced: False
	data_augmentation: True
	lr: 0.001
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0
env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  epoch         loss          mem_gb        step          step_time    
0.4972680523  0.4976425204  0.4996517919  0.4952850407  0.4963839931  0.4939991427  0.0000000000  0.7273724079  0.0000000000  0             2.7338812351 
0.1017784444  0.1039434205  0.2032999411  0.1935276468  0.9013231907  0.9063437634  0.3428510205  0.7133719891  0.0000000000  100           0.6229455066 
0.3435290336  0.3441920274  0.3907430224  0.3964852122  0.7290941233  0.7385340763  0.6857020410  0.6842233276  0.0000000000  200           0.6298244333 
Environment:
	Python: 3.6.13
	PyTorch: 1.10.1
	Torchvision: 0.11.2
	CUDA: None
	CUDNN: None
	NumPy: 1.19.2
	PIL: 8.3.1
Args:
	algorithm: IB_IRM
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: ColoredMNIST
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 64
	class_balanced: False
	data_augmentation: True
	ib_lambda: 100.0
	ib_penalty_anneal_iters: 500
	irm_lambda: 100.0
	irm_penalty_anneal_iters: 500
	lr: 0.001
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0
IB_penalty    IRM_penalty   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  epoch         loss          mem_gb        nll           step          step_time    
1.0733134747  0.0045223744  0.4972680523  0.4976425204  0.4996517919  0.4952850407  0.4963839931  0.4939991427  0.0000000000  0.7318947911  0.0000000000  0.7318947911  0             3.2178871632 
1.0551094925  0.0648330070  0.1033319049  0.1063009001  0.2050141962  0.1952421775  0.9008946269  0.9061294471  0.3428510205  0.7809252971  0.0000000000  0.7809252971  100           0.6436632609 
1.0125232083  0.0013152957  0.1498821513  0.1489498500  0.2431563722  0.2383197600  0.8831628007  0.8859837120  0.6857020410  0.6877849847  0.0000000000  0.6877849847  200           0.6324910331 
1.0094100124  0.0017736083  0.2879258624  0.2809687098  0.3506723094  0.3516930990  0.8066641667  0.8030432919  1.0285530616  0.6846862942  0.0000000000  0.6846862942  300           0.6451627326 
0.9994224346  0.0019769068  0.3112813370  0.3122588941  0.3696898270  0.3669095585  0.8292709059  0.8319759966  1.3714040821  0.6785876811  0.0000000000  0.6785876811  400           0.6281991482 
0.9803841496  0.0020424785  0.5027319477  0.5023574796  0.5003482081  0.5047149593  0.5036160069  0.5060008573  1.7142551026  1.6268212092  0.0000000000  1.6268212092  500           0.6344408703 
0.9800265414  0.0051596812  0.6100278552  0.6086583798  0.6187925216  0.6195885126  0.7248620560  0.7233176168  2.0571061231  99.193556289  0.0000000000  99.193556289  600           0.6364256620 
0.8940986568  0.0019581039  0.7032890508  0.7036005144  0.7063802432  0.7029575654  0.7167193443  0.7141020146  2.3999571436  90.217276001  0.0000000000  90.217276001  700           0.6076118612 
0.8695464242  0.0017075321  0.6872723377  0.6892413202  0.6948090213  0.6817402486  0.7437724326  0.7400342906  2.7428081641  87.720658493  0.0000000000  87.720658493  800           0.6713976669 
0.8595351827  0.0010506389  0.6930040711  0.6967423918  0.7007553437  0.6973853408  0.7489151979  0.7458208315  3.0856591847  86.647873382  0.0000000000  86.647873382  900           0.6544252443 
0.8511239773  0.0019109168  0.6995928862  0.7033861980  0.7066480956  0.6986712387  0.7523972786  0.7477496785  3.4285102052  85.887579879  0.0000000000  85.887579879  1000          0.6642250395 
0.8491902637  0.0016238041  0.6807370902  0.6793827690  0.6935233299  0.6877411059  0.7587721648  0.7516073725  3.7713612257  85.664679946  0.0000000000  85.664679946  1100          0.6247908282 
0.8284968668  0.0003597181  0.7152881937  0.7134590656  0.7211121230  0.7173167595  0.7486473456  0.7406772396  4.1142122462  83.454556884  0.0000000000  83.454556884  1200          0.6662091303 
0.8497807056  0.0002310774  0.7184486822  0.7216030862  0.7246477741  0.7153879126  0.7443081374  0.7359622803  4.4570632667  85.585686416  0.0000000000  85.585686416  1300          0.6754835677 
0.8362132770  0.0016397003  0.7080029998  0.7102443206  0.7165586329  0.7089584226  0.7533079766  0.7449635662  4.7999142872  84.360522155  0.0000000000  84.360522155  1400          0.6701456356 
0.8367953253  0.0003758608  0.7175916006  0.7183883412  0.7233620828  0.7143163309  0.7480044999  0.7370338620  5.1427653078  84.292957458  0.0000000000  84.292957458  1500          0.6501343775 
0.8170990604  -0.000004409  0.7242875509  0.7271753108  0.7310762308  0.7192456065  0.7478437885  0.7393913416  5.4856163283  82.271695175  0.0000000000  82.271695175  1600          0.6413075757 
0.8202097917  0.0006997720  0.7191450611  0.7162451779  0.7283441367  0.7138876982  0.7489687684  0.7366052293  5.8284673488  82.655493354  0.0000000000  82.655493354  1700          0.6541502690 
0.8177552032  0.0006318263  0.7241804157  0.7224603515  0.7331654792  0.7233176168  0.7476830771  0.7381054436  6.1713183693  82.401675872  0.0000000000  82.401675872  1800          0.6453522277 
0.8201202917  0.0009505253  0.7160381401  0.7177453922  0.7290941233  0.7119588513  0.7496651846  0.7396056580  6.5141693898  82.671909332  0.0000000000  82.671909332  1900          0.6547688293 
0.8190144247  0.0005075379  0.7163059781  0.7198885555  0.7288262710  0.7145306472  0.7560400707  0.7453921989  6.8570204103  82.516353530  0.0000000000  82.516353530  2000          0.6571406889 
0.8230286300  0.0018750917  0.7124491108  0.7143163309  0.7263084588  0.7130304329  0.7560936412  0.7423917703  7.1998714309  83.057503738  0.0000000000  83.057503738  2100          0.6580309367 
0.8091237909  0.0013101867  0.7135740304  0.7158165452  0.7281298548  0.7111015859  0.7604864199  0.7451778826  7.5427224514  81.600930023  0.0000000000  81.600930023  2200          0.6220207906 
0.8097166383  0.0016467585  0.7054853225  0.7083154736  0.7260406064  0.7053150450  0.7583436010  0.7393913416  7.8855734719  81.694385376  0.0000000000  81.694385376  2300          0.6983173823 
0.7990389967  0.0013020248  0.7081637026  0.7143163309  0.7275941501  0.7102443206  0.7636470777  0.7456065152  8.2284244924  80.584841232  0.0000000000  80.584841232  2400          0.7011113262 
0.8032224953  0.0013665678  0.7102528391  0.7098156880  0.7280762844  0.7023146164  0.7477366476  0.7273896271  8.5712755129  81.012499542  0.0000000000  81.012499542  2500          0.6981921840 
0.8034441251  0.0008197519  0.7111099207  0.7098156880  0.7332726201  0.7087441063  0.7629506616  0.7468924132  8.9141265335  80.980349960  0.0000000000  80.980349960  2600          0.7159915161 
0.8035417819  0.0022122986  0.7230554960  0.7207458208  0.7426474527  0.7196742392  0.7580757486  0.7396056580  9.2569775540  81.129494857  0.0000000000  81.129494857  2700          0.6986131644 
0.7968083858  0.0018139595  0.7091814870  0.7141020146  0.7324690631  0.7098156880  0.7570043392  0.7359622803  9.5998285745  80.411656189  0.0000000000  80.411656189  2800          0.6992913198 
Environment:
	Python: 3.6.13
	PyTorch: 1.10.1
	Torchvision: 0.11.2
	CUDA: None
	CUDNN: None
	NumPy: 1.19.2
	PIL: 8.3.1
Args:
	algorithm: IB_IRM
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: ColoredMNIST
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 64
	class_balanced: False
	data_augmentation: True
	ib_lambda: 100.0
	ib_penalty_anneal_iters: 500
	irm_lambda: 100.0
	irm_penalty_anneal_iters: 500
	lr: 0.001
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0
Environment:
	Python: 3.6.13
	PyTorch: 1.10.1
	Torchvision: 0.11.2
	CUDA: None
	CUDNN: None
	NumPy: 1.19.2
	PIL: 8.3.1
Args:
	algorithm: IB_IRM
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: ColoredMNIST
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 372
	class_balanced: False
	data_augmentation: True
	ib_lambda: 2.661822006035993
	ib_penalty_anneal_iters: 1222
	irm_lambda: 65030.52212762795
	irm_penalty_anneal_iters: 2
	lr: 0.00011906162909279594
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.5
	weight_decay: 0.0
Environment:
	Python: 3.6.13
	PyTorch: 1.10.1
	Torchvision: 0.11.2
	CUDA: None
	CUDNN: None
	NumPy: 1.19.2
	PIL: 8.3.1
Args:
	algorithm: IB_IRM
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: ColoredMNIST
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 372
	class_balanced: False
	data_augmentation: True
	ib_lambda: 2.661822006035993
	ib_penalty_anneal_iters: 1222
	irm_lambda: 65030.52212762795
	irm_penalty_anneal_iters: 2
	lr: 0.00011906162909279594
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.5
	weight_decay: 0.0
Namespace(algorithm='IB_IRM', checkpoint_freq=None, data_dir='./domainbed/data/', dataset='ColoredMNIST', holdout_fraction=0.2, hparams=None, hparams_seed=1, output_dir='train_output', save_model_every_checkpoint=False, seed=0, skip_model_save=False, steps=None, task='domain_generalization', test_envs=[0], trial_seed=0, uda_holdout_fraction=0)
Environment:
	Python: 3.6.13
	PyTorch: 1.10.1
	Torchvision: 0.11.2
	CUDA: None
	CUDNN: None
	NumPy: 1.19.2
	PIL: 8.3.1
Args:
	algorithm: IB_IRM
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: ColoredMNIST
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 372
	class_balanced: False
	data_augmentation: True
	ib_lambda: 2.661822006035993
	ib_penalty_anneal_iters: 1222
	irm_lambda: 65030.52212762795
	irm_penalty_anneal_iters: 2
	lr: 0.00011906162909279594
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.5
	weight_decay: 0.0
Namespace(algorithm='IB_IRM', checkpoint_freq=None, data_dir='./domainbed/data/', dataset='ColoredMNIST', holdout_fraction=0.2, hparams=None, hparams_seed=1, output_dir='train_output', save_model_every_checkpoint=False, seed=0, skip_model_save=False, steps=None, task='domain_generalization', test_envs=[0], trial_seed=0, uda_holdout_fraction=0)
Environment:
	Python: 3.6.13
	PyTorch: 1.10.1
	Torchvision: 0.11.2
	CUDA: None
	CUDNN: None
	NumPy: 1.19.2
	PIL: 8.3.1
Args:
	algorithm: IB_IRM
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: ColoredMNIST
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 1
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 372
	class_balanced: False
	data_augmentation: True
	ib_lambda: 2.661822006035993
	ib_penalty_anneal_iters: 1222
	irm_lambda: 65030.52212762795
	irm_penalty_anneal_iters: 2
	lr: 0.00011906162909279594
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.5
	weight_decay: 0.0
Namespace(algorithm='IB_IRM', checkpoint_freq=None, data_dir='./domainbed/data/', dataset='ColoredMNIST', holdout_fraction=0.2, hparams=None, hparams_seed=1, output_dir='train_output', save_model_every_checkpoint=False, seed=0, skip_model_save=False, steps=None, task='domain_generalization', test_envs=[0], trial_seed=0, uda_holdout_fraction=0)
Environment:
	Python: 3.6.13
	PyTorch: 1.10.1
	Torchvision: 0.11.2
	CUDA: None
	CUDNN: None
	NumPy: 1.19.2
	PIL: 8.3.1
Args:
	algorithm: IB_IRM
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: ColoredMNIST
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 64
	class_balanced: False
	data_augmentation: True
	ib_lambda: 100.0
	ib_penalty_anneal_iters: 500
	irm_lambda: 100.0
	irm_penalty_anneal_iters: 500
	lr: 0.001
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0
Namespace(algorithm='IB_IRM', checkpoint_freq=None, data_dir='./domainbed/data/', dataset='ColoredMNIST', holdout_fraction=0.2, hparams=None, hparams_seed=0, output_dir='train_output', save_model_every_checkpoint=False, seed=0, skip_model_save=False, steps=None, task='domain_generalization', test_envs=[0], trial_seed=0, uda_holdout_fraction=0)
Environment:
	Python: 3.6.13
	PyTorch: 1.10.1
	Torchvision: 0.11.2
	CUDA: None
	CUDNN: None
	NumPy: 1.19.2
	PIL: 8.3.1
Args:
	algorithm: IB_IRM
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: ColoredMNIST
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 64
	class_balanced: False
	data_augmentation: True
	ib_lambda: 100.0
	ib_penalty_anneal_iters: 500
	irm_lambda: 100.0
	irm_penalty_anneal_iters: 500
	lr: 0.001
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0
Namespace(algorithm='IB_IRM', checkpoint_freq=None, data_dir='./domainbed/data/', dataset='ColoredMNIST', holdout_fraction=0.2, hparams=None, hparams_seed=0, output_dir='train_output', save_model_every_checkpoint=False, seed=0, skip_model_save=False, steps=None, task='domain_generalization', test_envs=[0], trial_seed=0, uda_holdout_fraction=0)
Environment:
	Python: 3.6.13
	PyTorch: 1.10.1
	Torchvision: 0.11.2
	CUDA: None
	CUDNN: None
	NumPy: 1.19.2
	PIL: 8.3.1
Args:
	algorithm: IB_IRM
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: ColoredMNIST
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
Environment:
	Python: 3.6.13
	PyTorch: 1.10.1
	Torchvision: 0.11.2
	CUDA: None
	CUDNN: None
	NumPy: 1.19.2
	PIL: 8.3.1
Args:
	algorithm: IB_IRM
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: ColoredMNIST
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
Environment:
	Python: 3.6.13
	PyTorch: 1.10.1
	Torchvision: 0.11.2
	CUDA: None
	CUDNN: None
	NumPy: 1.19.2
	PIL: 8.3.1
Args:
	algorithm: IB_IRM
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: ColoredMNIST
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 64
	class_balanced: False
	data_augmentation: True
	ib_lambda: 100.0
	ib_penalty_anneal_iters: 500
	irm_lambda: 100.0
	irm_penalty_anneal_iters: 500
	lr: 0.001
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0
Environment:
	Python: 3.6.13
	PyTorch: 1.10.1
	Torchvision: 0.11.2
	CUDA: None
	CUDNN: None
	NumPy: 1.19.2
	PIL: 8.3.1
Args:
	algorithm: IB_IRM
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: ColoredMNIST
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 64
	class_balanced: False
	data_augmentation: True
	ib_lambda: 100.0
	ib_penalty_anneal_iters: 500
	irm_lambda: 100.0
	irm_penalty_anneal_iters: 500
	lr: 0.001
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0
0
Environment:
	Python: 3.6.13
	PyTorch: 1.10.1
	Torchvision: 0.11.2
	CUDA: None
	CUDNN: None
	NumPy: 1.19.2
	PIL: 8.3.1
Args:
	algorithm: IB_IRM
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: ColoredMNIST
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 64
	class_balanced: False
	data_augmentation: True
	ib_lambda: 100.0
	ib_penalty_anneal_iters: 500
	irm_lambda: 100.0
	irm_penalty_anneal_iters: 500
	lr: 0.001
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0
0
Environment:
	Python: 3.6.13
	PyTorch: 1.10.1
	Torchvision: 0.11.2
	CUDA: None
	CUDNN: None
	NumPy: 1.19.2
	PIL: 8.3.1
Args:
	algorithm: IB_IRM
	checkpoint_freq: None
	data_dir: ./domainbed/data/
	dataset: ColoredMNIST
	holdout_fraction: 0.2
	hparams: None
	hparams_seed: 0
	output_dir: train_output
	save_model_every_checkpoint: False
	seed: 0
	skip_model_save: False
	steps: None
	task: domain_generalization
	test_envs: [0]
	trial_seed: 0
	uda_holdout_fraction: 0
HParams:
	batch_size: 64
	class_balanced: False
	data_augmentation: True
	ib_lambda: 100.0
	ib_penalty_anneal_iters: 500
	irm_lambda: 100.0
	irm_penalty_anneal_iters: 500
	lr: 0.001
	nonlinear_classifier: False
	resnet18: False
	resnet_dropout: 0.0
	weight_decay: 0.0
IB_penalty    IRM_penalty   env0_in_acc   env0_out_acc  env1_in_acc   env1_out_acc  env2_in_acc   env2_out_acc  epoch         loss          mem_gb        nll           step          step_time    
1.0048068762  0.0000267890  0.4972680523  0.4976425204  0.4996517919  0.4952850407  0.4963839931  0.4939991427  0.0000000000  0.6957871914  0.0000000000  0.6957871914  0             1.8048951626 
